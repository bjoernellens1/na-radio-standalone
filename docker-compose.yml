## Compose file for CPU-only or portable runs. Use additional override files for device and GPU mapping.
services:
  naradio:
    build:
      context: .
      args:
        PYTORCH_BASE_IMAGE: ${PYTORCH_BASE_IMAGE:-pytorch/pytorch:2.2.2-cuda11.8-cudnn8-runtime}
    image: ghcr.io/bjoernellens1/na-radio-standalone:latest
    ports:
      - "5000:5000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - USE_ZLUDA=${USE_ZLUDA:-0}
      - HOST=0.0.0.0
      - PORT=5000
      - LABELS=person,car,dog,cat,tree
      - TORCH_HOME=/root/.cache/torch
      - HF_HOME=/root/.cache/huggingface
      - ENCODER_DEVICE=${ENCODER_DEVICE:-}
      - FORCE_GPU=${FORCE_GPU:-0}
      - MIN_CC=${MIN_CC:-7.0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # AMD GPU support via ZLUDA
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    # ipc: host # Optional: for shared memory
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5000/predictions" ]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - torch-cache:/root/.cache/torch
      - hf-cache:/root/.cache/huggingface

volumes:
  torch-cache:
  hf-cache:
